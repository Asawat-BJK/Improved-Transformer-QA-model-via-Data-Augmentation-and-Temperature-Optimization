{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a149231f",
   "metadata": {},
   "source": [
    "SYNONYM REPLACEMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c95153f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "squad = load_dataset(\"squad\", split=\"train\")\n",
    "squad = squad.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "\n",
    "from transformers import DefaultDataCollator\n",
    "\n",
    "data_collator = DefaultDataCollator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5071a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812e7734",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from random import shuffle\n",
    "from nltk.corpus import wordnet \n",
    "def get_only_chars(line):\n",
    "\n",
    "    clean_line = \"\"\n",
    "\n",
    "    line = line.replace(\"â€™\", \"\")\n",
    "    line = line.replace(\"'\", \"\")\n",
    "    line = line.replace(\"-\", \" \") #replace hyphens with spaces\n",
    "    line = line.replace(\"\\t\", \" \")\n",
    "    line = line.replace(\"\\n\", \" \")\n",
    "    line = line.lower()\n",
    "\n",
    "    for char in line:\n",
    "        if char in 'qwertyuiopasdfghjklzxcvbnm ':\n",
    "            clean_line += char\n",
    "        else:\n",
    "            clean_line += ' '\n",
    "\n",
    "    clean_line = re.sub(' +',' ',clean_line) #delete extra spaces\n",
    "    if clean_line[0] == ' ':\n",
    "        clean_line = clean_line[1:]\n",
    "    return clean_line\n",
    "\n",
    "stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', \n",
    "\t\t\t'ours', 'ourselves', 'you', 'your', 'yours', \n",
    "\t\t\t'yourself', 'yourselves', 'he', 'him', 'his', \n",
    "\t\t\t'himself', 'she', 'her', 'hers', 'herself', \n",
    "\t\t\t'it', 'its', 'itself', 'they', 'them', 'their', \n",
    "\t\t\t'theirs', 'themselves', 'what', 'which', 'who', \n",
    "\t\t\t'whom', 'this', 'that', 'these', 'those', 'am', \n",
    "\t\t\t'is', 'are', 'was', 'were', 'be', 'been', 'being', \n",
    "\t\t\t'have', 'has', 'had', 'having', 'do', 'does', 'did',\n",
    "\t\t\t'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or',\n",
    "\t\t\t'because', 'as', 'until', 'while', 'of', 'at', \n",
    "\t\t\t'by', 'for', 'with', 'about', 'against', 'between',\n",
    "\t\t\t'into', 'through', 'during', 'before', 'after', \n",
    "\t\t\t'above', 'below', 'to', 'from', 'up', 'down', 'in',\n",
    "\t\t\t'out', 'on', 'off', 'over', 'under', 'again', \n",
    "\t\t\t'further', 'then', 'once', 'here', 'there', 'when', \n",
    "\t\t\t'where', 'why', 'how', 'all', 'any', 'both', 'each', \n",
    "\t\t\t'few', 'more', 'most', 'other', 'some', 'such', 'no', \n",
    "\t\t\t'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', \n",
    "\t\t\t'very', 's', 't', 'can', 'will', 'just', 'don', \n",
    "\t\t\t'should', 'now', '']\n",
    "def synonym_replacement(words, n):\n",
    "    new_words = words.copy()\n",
    "    \n",
    "    # POS tagging\n",
    "    tags = pos_tag(words)\n",
    "    \n",
    "    # Candidate words: skip stopwords, proper nouns (NNP/NNPS), numbers\n",
    "    candidates = [\n",
    "        word for word, pos in tags\n",
    "        if word.lower() not in stop_words\n",
    "        and pos not in [\"NNP\", \"NNPS\"]   # skip proper nouns\n",
    "        and not word.isdigit()           # skip numbers\n",
    "    ]\n",
    "    \n",
    "    random.shuffle(candidates)\n",
    "    num_replaced = 0\n",
    "    \n",
    "    for word in candidates:\n",
    "        synonyms = get_synonyms(word)\n",
    "        if synonyms:\n",
    "            synonym = random.choice(synonyms)\n",
    "            new_words = [synonym if w == word else w for w in new_words]\n",
    "            num_replaced += 1\n",
    "        if num_replaced >= n:\n",
    "            break\n",
    "\n",
    "    # Reconstruct sentence\n",
    "    sentence = ' '.join(new_words)\n",
    "    new_words = sentence.split(' ')\n",
    "\n",
    "    return new_words\n",
    "\n",
    "def get_synonyms(word):\n",
    "\tsynonyms = set()\n",
    "\tfor syn in wordnet.synsets(word): \n",
    "\t\tfor l in syn.lemmas(): \n",
    "\t\t\tsynonym = l.name().replace(\"_\", \" \").replace(\"-\", \" \").lower()\n",
    "\t\t\tsynonym = \"\".join([char for char in synonym if char in ' qwertyuiopasdfghjklzxcvbnm'])\n",
    "\t\t\tsynonyms.add(synonym) \n",
    "\tif word in synonyms:\n",
    "\t\tsynonyms.remove(word)\n",
    "\treturn list(synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04abad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "from nltk.corpus import wordnet\n",
    "from copy import deepcopy\n",
    "\n",
    "# squad = load_dataset(\"squad\")\n",
    "examples_dict = squad['train'][:]\n",
    "\n",
    "# Convert column-oriented dict to list of row dicts\n",
    "examples = [dict(zip(examples_dict.keys(), values)) for values in zip(*examples_dict.values())]\n",
    "\n",
    "# Augmentation parameters\n",
    "N_AUGMENTATIONS_PER_SAMPLE = 1\n",
    "MAX_REPLACEMENTS = 1\n",
    "random.seed(1)\n",
    "\n",
    "augmented_samples = []\n",
    "\n",
    "for ex in examples:\n",
    "    original_q = ex['question']\n",
    "    cleaned = get_only_chars(original_q)\n",
    "    words = cleaned.split(' ')\n",
    "\n",
    "    # Create augmented versions\n",
    "    for i in range(N_AUGMENTATIONS_PER_SAMPLE):\n",
    "        augmented_words = synonym_replacement(words, MAX_REPLACEMENTS)\n",
    "        augmented_q = ' '.join(augmented_words)\n",
    "\n",
    "        new_sample = deepcopy(ex)\n",
    "        new_sample['id'] = ex['id'] + f\"_aug_{i}\"\n",
    "        new_sample['question'] = augmented_q\n",
    "        augmented_samples.append(new_sample)\n",
    "\n",
    "# Print results\n",
    "print(f\"Original examples: {len(examples)}\")\n",
    "print(f\"Augmented examples: {len(augmented_samples)}\\n\")\n",
    "\n",
    "for i, ex in enumerate(examples[1:5]):\n",
    "    print(f\"ðŸŸ© Original:  {ex['question']}\")\n",
    "    for aug in augmented_samples:\n",
    "        if aug['id'].startswith(ex['id']):\n",
    "            print(f\"ðŸŸ¦ Augmented: {aug['question']}\")\n",
    "    print('-' * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1256e948",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "\n",
    "# Convert augmented samples (list of dicts) to a Dataset\n",
    "augmented_train_dataset = Dataset.from_pandas(pd.DataFrame(augmented_samples))\n",
    "\n",
    "# Keep original test dataset\n",
    "test_dataset = squad['test']\n",
    "\n",
    "# Create new DatasetDict\n",
    "augmented_squad = DatasetDict({\n",
    "    'train': augmented_train_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "# Check\n",
    "print(squad['train']['question'][4])\n",
    "print(augmented_squad['train']['question'][4])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fuckingneuralnetwork",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
