{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e44f8cbd",
   "metadata": {},
   "source": [
    "Back-Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88746eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "squad = load_dataset(\"squad\", split=\"train\")\n",
    "squad = squad.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "\n",
    "from transformers import DefaultDataCollator\n",
    "\n",
    "data_collator = DefaultDataCollator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "238e8e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "augmented_squad = copy.deepcopy(squad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a72919cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# --- LOAD MODELS + TOKENIZERS ---\n",
    "\n",
    "src_to_tgt_model_name = \"Helsinki-NLP/opus-mt-en-de\"\n",
    "tgt_to_src_model_name = \"Helsinki-NLP/opus-mt-de-en\"\n",
    "\n",
    "tokenizer_src2tgt = MarianTokenizer.from_pretrained(src_to_tgt_model_name, use_fast=True)\n",
    "model_src2tgt = MarianMTModel.from_pretrained(src_to_tgt_model_name).to(device)\n",
    "\n",
    "tokenizer_tgt2src = MarianTokenizer.from_pretrained(tgt_to_src_model_name, use_fast=True)\n",
    "model_tgt2src = MarianMTModel.from_pretrained(tgt_to_src_model_name).to(device)\n",
    "\n",
    "def translate_batch(sentences, tokenizer, model, max_length=256):\n",
    "    \"\"\"\n",
    "    Translates a batch of sentences using a MarianMT model.\n",
    "    \"\"\"\n",
    "    encoded = tokenizer(\n",
    "    sentences,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=max_length\n",
    "    ).to(device)\n",
    "\n",
    "\n",
    "    generated_tokens = model.generate(\n",
    "        **encoded,\n",
    "        max_length=max_length\n",
    "    )\n",
    "\n",
    "    return [tokenizer.decode(t, skip_special_tokens=True) for t in generated_tokens]\n",
    "\n",
    "\n",
    "def back_translate(sentences_list):\n",
    "    \"\"\"\n",
    "    #Performs back-translation English → German → English.\n",
    "    #Returns paraphrased English sentences.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. English → German\n",
    "        translated_batch = translate_batch(sentences_list, tokenizer_src2tgt, model_src2tgt)\n",
    "\n",
    "\n",
    "        # 2. German → English\n",
    "        back_translated_batch = translate_batch(translated_batch, tokenizer_tgt2src, model_tgt2src)\n",
    "\n",
    "        return back_translated_batch\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during back-translation: {e}\")\n",
    "        return [None] * len(sentences_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70512cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch augmentation on 70079 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Translating:   0%|          | 20/8760 [00:14<1:44:07,  1.40it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     23\u001b[39m batch_records = original_list[i:i+BATCH_SIZE]\n\u001b[32m     24\u001b[39m batch_questions = [r[\u001b[33m'\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m batch_records]\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m paraphrased_questions = \u001b[43mback_translate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_questions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m j, record \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch_records):\n\u001b[32m     29\u001b[39m     p = paraphrased_questions[j]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 47\u001b[39m, in \u001b[36mback_translate\u001b[39m\u001b[34m(sentences_list)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     42\u001b[39m \u001b[33;03m#Performs back-translation English → German → English.\u001b[39;00m\n\u001b[32m     43\u001b[39m \u001b[33;03m#Returns paraphrased English sentences.\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     46\u001b[39m     \u001b[38;5;66;03m# 1. English → German\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     translated_batch = \u001b[43mtranslate_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer_src2tgt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_src2tgt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m     \u001b[38;5;66;03m# 2. German → English\u001b[39;00m\n\u001b[32m     51\u001b[39m     back_translated_batch = translate_batch(translated_batch, tokenizer_tgt2src, model_tgt2src)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mtranslate_batch\u001b[39m\u001b[34m(sentences, tokenizer, model, max_length)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[33;03mTranslates a batch of sentences using a MarianMT model.\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     23\u001b[39m encoded = tokenizer(\n\u001b[32m     24\u001b[39m sentences,\n\u001b[32m     25\u001b[39m return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     28\u001b[39m max_length=max_length\n\u001b[32m     29\u001b[39m ).to(device)\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m generated_tokens = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mencoded\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [tokenizer.decode(t, skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m generated_tokens]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jettanat\\anaconda3\\envs\\fuckingneuralnetwork\\Lib\\site-packages\\torch\\utils\\_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jettanat\\anaconda3\\envs\\fuckingneuralnetwork\\Lib\\site-packages\\transformers\\generation\\utils.py:2564\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2561\u001b[39m model_kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m] = generation_config.use_cache\n\u001b[32m   2563\u001b[39m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2564\u001b[39m result = \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2565\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2566\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2567\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2568\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2569\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2570\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2571\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2572\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2574\u001b[39m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[32m   2575\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2576\u001b[39m     generation_config.return_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2577\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2578\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result.past_key_values, \u001b[33m\"\u001b[39m\u001b[33mto_legacy_cache\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2579\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jettanat\\anaconda3\\envs\\fuckingneuralnetwork\\Lib\\site-packages\\transformers\\generation\\utils.py:3340\u001b[39m, in \u001b[36mGenerationMixin._beam_search\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[39m\n\u001b[32m   3335\u001b[39m next_token_hits_stopping_criteria = \u001b[38;5;28mself\u001b[39m._unflatten_beam_dim(\n\u001b[32m   3336\u001b[39m     next_token_hits_stopping_criteria, batch_size, beams_to_keep\n\u001b[32m   3337\u001b[39m )\n\u001b[32m   3339\u001b[39m \u001b[38;5;66;03m# e. Get the non-finished running `num_beams` sequences for the next generation step\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3340\u001b[39m running_sequences, running_beam_scores, running_beam_indices = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_running_beams_for_next_iteration\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3341\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtopk_log_probs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtopk_log_probs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3342\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtopk_running_sequences\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtopk_running_sequences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3343\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtopk_running_beam_indices\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtopk_running_beam_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3344\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnext_token_hits_stopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnext_token_hits_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3345\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3346\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3348\u001b[39m \u001b[38;5;66;03m# f. Update the completed beams if a new high score in a finished sequence is found\u001b[39;00m\n\u001b[32m   3349\u001b[39m sequences, beam_scores, beam_indices, is_sent_finished = \u001b[38;5;28mself\u001b[39m._update_finished_beams(\n\u001b[32m   3350\u001b[39m     sequences=sequences,\n\u001b[32m   3351\u001b[39m     topk_running_sequences=topk_running_sequences,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3364\u001b[39m     early_stopping=early_stopping,\n\u001b[32m   3365\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jettanat\\anaconda3\\envs\\fuckingneuralnetwork\\Lib\\site-packages\\transformers\\generation\\utils.py:3048\u001b[39m, in \u001b[36mGenerationMixin._get_running_beams_for_next_iteration\u001b[39m\u001b[34m(self, topk_log_probs, topk_running_sequences, topk_running_beam_indices, next_token_hits_stopping_criteria, num_beams)\u001b[39m\n\u001b[32m   3042\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3043\u001b[39m \u001b[33;03mGiven the top-K continuations, their scores, and whether they hit a stopping criteria, select the\u001b[39;00m\n\u001b[32m   3044\u001b[39m \u001b[33;03mbest non-finished beams to continue beam search in the next iteration.\u001b[39;00m\n\u001b[32m   3045\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3046\u001b[39m \u001b[38;5;66;03m# To prevent these just finished sequences from being used in subsequent iterations, set their log probs\u001b[39;00m\n\u001b[32m   3047\u001b[39m \u001b[38;5;66;03m# to a very large negative value\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3048\u001b[39m topk_running_log_probs = topk_log_probs + \u001b[43mnext_token_hits_stopping_criteria\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m * -\u001b[32m1.0e9\u001b[39m\n\u001b[32m   3050\u001b[39m next_topk_indices = torch.topk(topk_running_log_probs, k=num_beams)[\u001b[32m1\u001b[39m]\n\u001b[32m   3051\u001b[39m running_sequences = \u001b[38;5;28mself\u001b[39m._gather_beams(topk_running_sequences, next_topk_indices)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import copy\n",
    "from datasets import Dataset, concatenate_datasets\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "BATCH_SIZE = 8\n",
    "LANG = \"de\"\n",
    "SAVE_EVERY = 100   # save after every 10 batches\n",
    "\n",
    "# Prepare\n",
    "original_train_dataset = squad['train']\n",
    "original_list = original_train_dataset.to_list()\n",
    "total_records = len(original_list)\n",
    "\n",
    "new_augmented_datasets = []   # temporary storage\n",
    "batch_cache = []              # in-memory batch cache\n",
    "\n",
    "print(f\"Starting batch augmentation on {total_records} samples...\")\n",
    "\n",
    "for b, i in enumerate(tqdm(range(0, total_records, BATCH_SIZE), desc=\"Batch Translating\")):\n",
    "    batch_records = original_list[i:i+BATCH_SIZE]\n",
    "    batch_questions = [r['question'] for r in batch_records]\n",
    "    \n",
    "    paraphrased_questions = back_translate(batch_questions)\n",
    "    \n",
    "    for j, record in enumerate(batch_records):\n",
    "        p = paraphrased_questions[j]\n",
    "        if p and isinstance(p, str):\n",
    "            new_record = dict(record)  # shallow copy only\n",
    "            global_index = i + j\n",
    "            new_record['id'] = f\"{record['id']}-aug-{LANG}-{global_index}\"\n",
    "            new_record['question'] = p\n",
    "            batch_cache.append(new_record)\n",
    "    \n",
    "    # Flush to Dataset every SAVE_EVERY batches\n",
    "    if (b + 1) % SAVE_EVERY == 0 or i + BATCH_SIZE >= total_records:\n",
    "        temp_dataset = Dataset.from_list(batch_cache)\n",
    "        new_augmented_datasets.append(temp_dataset)\n",
    "        batch_cache.clear()  # free memory\n",
    "        #time.sleep(2)  # optional cooldown\n",
    "\n",
    "print(\"Merging all temporary datasets...\")\n",
    "new_augmented_dataset = concatenate_datasets(new_augmented_datasets)\n",
    "print(\"✅ Done! Final size:\", len(new_augmented_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4808b88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_squad['train'] = new_augmented_dataset\n",
    "print(augmented_squad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fuckingneuralnetwork",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
