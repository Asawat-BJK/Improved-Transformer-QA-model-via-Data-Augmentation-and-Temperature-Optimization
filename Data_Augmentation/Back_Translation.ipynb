{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e44f8cbd",
   "metadata": {},
   "source": [
    "Back-Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88746eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "squad = load_dataset(\"squad\", split=\"train\")\n",
    "squad = squad.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "\n",
    "from transformers import DefaultDataCollator\n",
    "\n",
    "data_collator = DefaultDataCollator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "238e8e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "augmented_squad = copy.deepcopy(squad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a72919cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# --- LOAD MODELS + TOKENIZERS ---\n",
    "\n",
    "src_to_tgt_model_name = \"Helsinki-NLP/opus-mt-en-de\"\n",
    "tgt_to_src_model_name = \"Helsinki-NLP/opus-mt-de-en\"\n",
    "\n",
    "tokenizer_src2tgt = MarianTokenizer.from_pretrained(src_to_tgt_model_name, use_fast=True)\n",
    "model_src2tgt = MarianMTModel.from_pretrained(src_to_tgt_model_name).to(device)\n",
    "\n",
    "tokenizer_tgt2src = MarianTokenizer.from_pretrained(tgt_to_src_model_name, use_fast=True)\n",
    "model_tgt2src = MarianMTModel.from_pretrained(tgt_to_src_model_name).to(device)\n",
    "\n",
    "def translate_batch(sentences, tokenizer, model, max_length=256):\n",
    "    \"\"\"\n",
    "    Translates a batch of sentences using a MarianMT model.\n",
    "    \"\"\"\n",
    "    encoded = tokenizer(\n",
    "    sentences,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=max_length\n",
    "    ).to(device)\n",
    "\n",
    "\n",
    "    generated_tokens = model.generate(\n",
    "        **encoded,\n",
    "        max_length=max_length\n",
    "    )\n",
    "\n",
    "    return [tokenizer.decode(t, skip_special_tokens=True) for t in generated_tokens]\n",
    "\n",
    "\n",
    "def back_translate(sentences_list):\n",
    "    \"\"\"\n",
    "    #Performs back-translation English → German → English.\n",
    "    #Returns paraphrased English sentences.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. English → German\n",
    "        translated_batch = translate_batch(sentences_list, tokenizer_src2tgt, model_src2tgt)\n",
    "\n",
    "\n",
    "        # 2. German → English\n",
    "        back_translated_batch = translate_batch(translated_batch, tokenizer_tgt2src, model_tgt2src)\n",
    "\n",
    "        return back_translated_batch\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during back-translation: {e}\")\n",
    "        return [None] * len(sentences_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70512cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from datasets import Dataset, concatenate_datasets\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "BATCH_SIZE = 8\n",
    "LANG = \"de\"\n",
    "SAVE_EVERY = 100   # save after every 10 batches\n",
    "\n",
    "# Prepare\n",
    "original_train_dataset = squad['train']\n",
    "original_list = original_train_dataset.to_list()\n",
    "total_records = len(original_list)\n",
    "\n",
    "new_augmented_datasets = []   # temporary storage\n",
    "batch_cache = []              # in-memory batch cache\n",
    "\n",
    "print(f\"Starting batch augmentation on {total_records} samples...\")\n",
    "\n",
    "for b, i in enumerate(tqdm(range(0, total_records, BATCH_SIZE), desc=\"Batch Translating\")):\n",
    "    batch_records = original_list[i:i+BATCH_SIZE]\n",
    "    batch_questions = [r['question'] for r in batch_records]\n",
    "    \n",
    "    paraphrased_questions = back_translate(batch_questions)\n",
    "    \n",
    "    for j, record in enumerate(batch_records):\n",
    "        p = paraphrased_questions[j]\n",
    "        if p and isinstance(p, str):\n",
    "            new_record = dict(record)  # shallow copy only\n",
    "            global_index = i + j\n",
    "            new_record['id'] = f\"{record['id']}-aug-{LANG}-{global_index}\"\n",
    "            new_record['question'] = p\n",
    "            batch_cache.append(new_record)\n",
    "    \n",
    "    # Flush to Dataset every SAVE_EVERY batches\n",
    "    if (b + 1) % SAVE_EVERY == 0 or i + BATCH_SIZE >= total_records:\n",
    "        temp_dataset = Dataset.from_list(batch_cache)\n",
    "        new_augmented_datasets.append(temp_dataset)\n",
    "        batch_cache.clear()  # free memory\n",
    "        #time.sleep(2)  # optional cooldown\n",
    "\n",
    "print(\"Merging all temporary datasets...\")\n",
    "new_augmented_dataset = concatenate_datasets(new_augmented_datasets)\n",
    "print(\"Final size:\", len(new_augmented_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4808b88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_squad['train'] = new_augmented_dataset\n",
    "print(augmented_squad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fuckingneuralnetwork",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
