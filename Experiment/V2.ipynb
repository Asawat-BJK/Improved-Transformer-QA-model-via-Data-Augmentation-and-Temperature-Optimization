{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a2655ef1",
      "metadata": {
        "id": "a2655ef1",
        "outputId": "2ed74b49-5671-49a9-f7d1-a39878c6f08c"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "squad = load_dataset(\"squad\", split=\"train\")\n",
        "squad = squad.train_test_split(test_size=0.2, seed=42)\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
        "\n",
        "from transformers import DefaultDataCollator\n",
        "\n",
        "data_collator = DefaultDataCollator()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "8a0200de",
      "metadata": {},
      "outputs": [],
      "source": [
        "import copy\n",
        "augmented_squad = copy.deepcopy(squad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "dd11fe3c",
      "metadata": {},
      "outputs": [],
      "source": [
        "import copy\n",
        "from deep_translator import GoogleTranslator\n",
        "from datasets import concatenate_datasets, Dataset\n",
        "from tqdm import tqdm \n",
        "\n",
        "\n",
        "\n",
        "def translate_batch_and_back(sentences_list, intermediate_lang):\n",
        "    \"\"\"\n",
        "    Performs batch back-translation.\n",
        "    Returns a list of paraphrased strings or None for failed batches.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # 1. Translate the entire batch to the intermediate language\n",
        "        translated_batch = GoogleTranslator(\n",
        "            source='en', \n",
        "            target=intermediate_lang\n",
        "        ).translate_batch(sentences_list)\n",
        "        \n",
        "        # 2. Translate the entire batch back to the target language (English)\n",
        "        back_translated_batch = GoogleTranslator(\n",
        "            source=intermediate_lang, \n",
        "            target='en'\n",
        "        ).translate_batch(translated_batch)\n",
        "        \n",
        "        return back_translated_batch\n",
        "    except Exception as e:\n",
        "        print(f\"Error during batch translation: {e}\")\n",
        "        # If the batch fails, return a list of None to skip those records\n",
        "        return [None] * len(sentences_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "3d90293b",
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# --- LOAD MODEL + TOKENIZER ---\u001b[39;00m\n\u001b[32m      8\u001b[39m model_name = \u001b[33m\"\u001b[39m\u001b[33mfacebook/m2m100_418M\u001b[39m\u001b[33m\"\u001b[39m   \u001b[38;5;66;03m# Use 418M (faster) or 1.2B for higher quality\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m tokenizer = \u001b[43mM2M100Tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m model = M2M100ForConditionalGeneration.from_pretrained(model_name).to(device).half()\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtranslate_batch\u001b[39m(sentences, src_lang, tgt_lang, max_length=\u001b[32m256\u001b[39m):\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jettanat\\anaconda3\\envs\\fuckingneuralnetwork\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2097\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2094\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2095\u001b[39m         logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2097\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2098\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2099\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2100\u001b[39m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2101\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2102\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2103\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2104\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2105\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2106\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2107\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2108\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2109\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jettanat\\anaconda3\\envs\\fuckingneuralnetwork\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2343\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._from_pretrained\u001b[39m\u001b[34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2341\u001b[39m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[32m   2342\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2343\u001b[39m     tokenizer = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2344\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m import_protobuf_decode_error():\n\u001b[32m   2345\u001b[39m     logger.info(\n\u001b[32m   2346\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUnable to load tokenizer model from SPM, loading from TikToken will be attempted instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2347\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(Google protobuf error: Tried to load SPM model with non-SPM vocab file).\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2348\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jettanat\\anaconda3\\envs\\fuckingneuralnetwork\\Lib\\site-packages\\transformers\\models\\m2m_100\\tokenization_m2m_100.py:162\u001b[39m, in \u001b[36mM2M100Tokenizer.__init__\u001b[39m\u001b[34m(self, vocab_file, spm_file, src_lang, tgt_lang, bos_token, eos_token, sep_token, pad_token, unk_token, language_codes, sp_model_kwargs, num_madeup_words, **kwargs)\u001b[39m\n\u001b[32m    158\u001b[39m \u001b[38;5;28mself\u001b[39m.cur_lang_id = \u001b[38;5;28mself\u001b[39m.get_lang_id(\u001b[38;5;28mself\u001b[39m._src_lang)\n\u001b[32m    160\u001b[39m \u001b[38;5;28mself\u001b[39m.num_madeup_words = num_madeup_words\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m    \u001b[49m\u001b[43msrc_lang\u001b[49m\u001b[43m=\u001b[49m\u001b[43msrc_lang\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtgt_lang\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtgt_lang\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbos_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m    \u001b[49m\u001b[43meos_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43meos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m    \u001b[49m\u001b[43msep_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43msep_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m    \u001b[49m\u001b[43munk_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43munk_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpad_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlanguage_codes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlanguage_codes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m    \u001b[49m\u001b[43msp_model_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msp_model_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m    \u001b[49m\u001b[43madditional_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madditional_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_madeup_words\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_madeup_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28mself\u001b[39m.set_src_lang_special_tokens(\u001b[38;5;28mself\u001b[39m._src_lang)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jettanat\\anaconda3\\envs\\fuckingneuralnetwork\\Lib\\site-packages\\transformers\\tokenization_utils.py:438\u001b[39m, in \u001b[36mPreTrainedTokenizer.__init__\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    434\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(**kwargs)\n\u001b[32m    436\u001b[39m \u001b[38;5;66;03m# 4. If some of the special tokens are not part of the vocab, we add them, at the end.\u001b[39;00m\n\u001b[32m    437\u001b[39m \u001b[38;5;66;03m# the order of addition is the same as self.SPECIAL_TOKENS_ATTRIBUTES following `tokenizers`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m438\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_add_tokens\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mall_special_tokens_extended\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_added_tokens_encoder\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspecial_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    441\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[38;5;28mself\u001b[39m._decode_use_source_tokenizer = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jettanat\\anaconda3\\envs\\fuckingneuralnetwork\\Lib\\site-packages\\transformers\\tokenization_utils.py:546\u001b[39m, in \u001b[36mPreTrainedTokenizer._add_tokens\u001b[39m\u001b[34m(self, new_tokens, special_tokens)\u001b[39m\n\u001b[32m    544\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m added_tokens\n\u001b[32m    545\u001b[39m \u001b[38;5;66;03m# TODO this is fairly slow to improve!\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m546\u001b[39m current_vocab = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.copy()\n\u001b[32m    547\u001b[39m new_idx = \u001b[38;5;28mlen\u001b[39m(current_vocab)  \u001b[38;5;66;03m# only call this once, len gives the last index + 1\u001b[39;00m\n\u001b[32m    548\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m new_tokens:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jettanat\\anaconda3\\envs\\fuckingneuralnetwork\\Lib\\site-packages\\transformers\\models\\m2m_100\\tokenization_m2m_100.py:183\u001b[39m, in \u001b[36mM2M100Tokenizer.get_vocab\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_vocab\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mdict\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m     vocab = {\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_ids_to_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m: i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.vocab_size)}\n\u001b[32m    184\u001b[39m     vocab.update(\u001b[38;5;28mself\u001b[39m.added_tokens_encoder)\n\u001b[32m    185\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m vocab\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jettanat\\anaconda3\\envs\\fuckingneuralnetwork\\Lib\\site-packages\\transformers\\tokenization_utils.py:1064\u001b[39m, in \u001b[36mPreTrainedTokenizer.convert_ids_to_tokens\u001b[39m\u001b[34m(self, ids, skip_special_tokens)\u001b[39m\n\u001b[32m   1062\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._added_tokens_decoder[ids].content\n\u001b[32m   1063\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1064\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_id_to_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43mids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1065\u001b[39m tokens = []\n\u001b[32m   1066\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m ids:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jettanat\\anaconda3\\envs\\fuckingneuralnetwork\\Lib\\site-packages\\transformers\\models\\m2m_100\\tokenization_m2m_100.py:208\u001b[39m, in \u001b[36mM2M100Tokenizer._convert_id_to_token\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.id_to_lang_token:\n\u001b[32m    207\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.id_to_lang_token[index]\n\u001b[32m--> \u001b[39m\u001b[32m208\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.decoder.get(index, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43munk_token\u001b[49m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jettanat\\anaconda3\\envs\\fuckingneuralnetwork\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1111\u001b[39m, in \u001b[36mSpecialTokensMixin.__getattr__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m key_is_special_id:\n\u001b[32m   1109\u001b[39m     key_without_id = key[:-\u001b[32m3\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m key.endswith(\u001b[33m\"\u001b[39m\u001b[33m_ids\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m key[:-\u001b[32m4\u001b[39m]\n\u001b[32m-> \u001b[39m\u001b[32m1111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.\u001b[34m__dict__\u001b[39m.get(\u001b[33m\"\u001b[39m\u001b[33m_special_tokens_map\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28;43many\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m   1112\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__dict__\u001b[39;49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m_special_tokens_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_without_id\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m   1113\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   1114\u001b[39m     _special_tokens_map = \u001b[38;5;28mself\u001b[39m.\u001b[34m__dict__\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33m_special_tokens_map\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1115\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m key_is_special_id:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jettanat\\anaconda3\\envs\\fuckingneuralnetwork\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1111\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m   1108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m key_is_special_id:\n\u001b[32m   1109\u001b[39m     key_without_id = key[:-\u001b[32m3\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m key.endswith(\u001b[33m\"\u001b[39m\u001b[33m_ids\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m key[:-\u001b[32m4\u001b[39m]\n\u001b[32m-> \u001b[39m\u001b[32m1111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.\u001b[34m__dict__\u001b[39m.get(\u001b[33m\"\u001b[39m\u001b[33m_special_tokens_map\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[32m   1112\u001b[39m     name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.\u001b[34m__dict__\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33m_special_tokens_map\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m [key, key_without_id]\n\u001b[32m   1113\u001b[39m ):\n\u001b[32m   1114\u001b[39m     _special_tokens_map = \u001b[38;5;28mself\u001b[39m.\u001b[34m__dict__\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33m_special_tokens_map\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1115\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m key_is_special_id:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# --- LOAD MODEL + TOKENIZER ---\n",
        "model_name = \"facebook/m2m100_418M\"   # Use 418M (faster) or 1.2B for higher quality\n",
        "tokenizer = M2M100Tokenizer.from_pretrained(model_name)\n",
        "model = M2M100ForConditionalGeneration.from_pretrained(model_name).to(device).half()\n",
        "\n",
        "\n",
        "def translate_batch(sentences, src_lang, tgt_lang, max_length=256):\n",
        "    \"\"\"\n",
        "    Translates a batch of sentences from src_lang → tgt_lang using M2M100.\n",
        "    \"\"\"\n",
        "    tokenizer.src_lang = src_lang\n",
        "    encoded = tokenizer(\n",
        "        sentences,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=max_length\n",
        "    ).to(device)\n",
        "\n",
        "    # Force decoder to generate in target language\n",
        "    generated_tokens = model.generate(\n",
        "        **encoded,\n",
        "        forced_bos_token_id=tokenizer.get_lang_id(tgt_lang),\n",
        "        max_length=max_length\n",
        "    )\n",
        "\n",
        "    return [tokenizer.decode(t, skip_special_tokens=True) for t in generated_tokens]\n",
        "\n",
        "\n",
        "def translate_batch_and_back(sentences_list, intermediate_lang=LANG):\n",
        "    \"\"\"\n",
        "    Performs back-translation with M2M100 (English → LANG → English).\n",
        "    Returns paraphrased English sentences.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # 1. English → Intermediate Language\n",
        "        translated_batch = translate_batch(sentences_list, \"en\", intermediate_lang)\n",
        "\n",
        "        # 2. Intermediate Language → English\n",
        "        back_translated_batch = translate_batch(translated_batch, intermediate_lang, \"en\")\n",
        "\n",
        "        return back_translated_batch\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during batch translation: {e}\")\n",
        "        return [None] * len(sentences_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "c38d5c59",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# --- LOAD MODELS + TOKENIZERS ---\n",
        "\n",
        "src_to_tgt_model_name = \"Helsinki-NLP/opus-mt-en-de\"\n",
        "tgt_to_src_model_name = \"Helsinki-NLP/opus-mt-de-en\"\n",
        "\n",
        "tokenizer_src2tgt = MarianTokenizer.from_pretrained(src_to_tgt_model_name, use_fast=True)\n",
        "model_src2tgt = MarianMTModel.from_pretrained(src_to_tgt_model_name).to(device)\n",
        "\n",
        "tokenizer_tgt2src = MarianTokenizer.from_pretrained(tgt_to_src_model_name, use_fast=True)\n",
        "model_tgt2src = MarianMTModel.from_pretrained(tgt_to_src_model_name).to(device)\n",
        "\n",
        "def translate_batch(sentences, tokenizer, model, max_length=256):\n",
        "    \"\"\"\n",
        "    Translates a batch of sentences using a MarianMT model.\n",
        "    \"\"\"\n",
        "    encoded = tokenizer(\n",
        "    sentences,\n",
        "    return_tensors=\"pt\",\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=max_length\n",
        "    ).to(device)\n",
        "\n",
        "\n",
        "    generated_tokens = model.generate(\n",
        "        **encoded,\n",
        "        max_length=max_length\n",
        "    )\n",
        "\n",
        "    return [tokenizer.decode(t, skip_special_tokens=True) for t in generated_tokens]\n",
        "\n",
        "\n",
        "def back_translate(sentences_list):\n",
        "    \"\"\"\n",
        "    #Performs back-translation English → German → English.\n",
        "    #Returns paraphrased English sentences.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # 1. English → German\n",
        "        translated_batch = translate_batch(sentences_list, tokenizer_src2tgt, model_src2tgt)\n",
        "\n",
        "\n",
        "        # 2. German → English\n",
        "        back_translated_batch = translate_batch(translated_batch, tokenizer_tgt2src, model_tgt2src)\n",
        "\n",
        "        return back_translated_batch\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during back-translation: {e}\")\n",
        "        return [None] * len(sentences_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "90d4fb23",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting batch augmentation on 70079 samples...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Batch Translating: 100%|██████████| 8760/8760 [1:15:21<00:00,  1.94it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Merging all temporary datasets...\n",
            "✅ Done! Final size: 70079\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import copy\n",
        "from datasets import Dataset, concatenate_datasets\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Configuration\n",
        "BATCH_SIZE = 8\n",
        "LANG = \"de\"\n",
        "SAVE_EVERY = 100   # save after every 10 batches\n",
        "\n",
        "# Prepare\n",
        "original_train_dataset = squad['train']\n",
        "original_list = original_train_dataset.to_list()\n",
        "total_records = len(original_list)\n",
        "\n",
        "new_augmented_datasets = []   # temporary storage\n",
        "batch_cache = []              # in-memory batch cache\n",
        "\n",
        "print(f\"Starting batch augmentation on {total_records} samples...\")\n",
        "\n",
        "for b, i in enumerate(tqdm(range(0, total_records, BATCH_SIZE), desc=\"Batch Translating\")):\n",
        "    batch_records = original_list[i:i+BATCH_SIZE]\n",
        "    batch_questions = [r['question'] for r in batch_records]\n",
        "    \n",
        "    paraphrased_questions = back_translate(batch_questions)\n",
        "    \n",
        "    for j, record in enumerate(batch_records):\n",
        "        p = paraphrased_questions[j]\n",
        "        if p and isinstance(p, str):\n",
        "            new_record = dict(record)  # shallow copy only\n",
        "            global_index = i + j\n",
        "            new_record['id'] = f\"{record['id']}-aug-{LANG}-{global_index}\"\n",
        "            new_record['question'] = p\n",
        "            batch_cache.append(new_record)\n",
        "    \n",
        "    # Flush to Dataset every SAVE_EVERY batches\n",
        "    if (b + 1) % SAVE_EVERY == 0 or i + BATCH_SIZE >= total_records:\n",
        "        temp_dataset = Dataset.from_list(batch_cache)\n",
        "        new_augmented_datasets.append(temp_dataset)\n",
        "        batch_cache.clear()  # free memory\n",
        "        #time.sleep(2)  # optional cooldown\n",
        "\n",
        "print(\"Merging all temporary datasets...\")\n",
        "new_augmented_dataset = concatenate_datasets(new_augmented_datasets)\n",
        "print(\"✅ Done! Final size:\", len(new_augmented_dataset))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "7aed182f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
            "        num_rows: 70079\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
            "        num_rows: 17520\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "augmented_squad['train'] = new_augmented_dataset\n",
        "print(augmented_squad)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1c46d6b",
      "metadata": {},
      "source": [
        "save df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "89304d89",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DataFrame successfully saved to augmented_squad.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd \n",
        "\n",
        "# Define the filename\n",
        "output_file_path = \"augmented_squad.csv\"\n",
        "\n",
        "augmented_squad['train'].to_pandas().to_csv(output_file_path, index=False)\n",
        "\n",
        "print(f\"DataFrame successfully saved to {output_file_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22a6170d",
      "metadata": {},
      "source": [
        "check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "edf7de1e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "How many households are there in Tucson from 2010 on?\n",
            "How many households are there in Tucson as of 2010?\n",
            "The spread of Buddhism led to what great effort?\n",
            "Buddhism's spread led to what large-scale effort?\n",
            "In which year was the Jolt Hall of Fame recorded by DeveloperWorks?\n",
            "The Jolt Hall of Fame inducted DeveloperWorks in what year?\n",
            "What year was Ali killed?\n",
            "In what year was Ali killed?\n",
            "The pub \"The Bag o'Nails\" was a corruption of what word?\n",
            "The pub \"The Bag o'Nails\" was a corruption of what word?\n",
            "Madonna was born in what religion?\n",
            "Madonna was born to which religion?\n",
            "What conflict did the Corporate Library find with Comcast's board?\n",
            "What conflict did Corporate Library note with Comcast's Board?\n",
            "Which state had the most pilots per capita than any other U.S. state?\n",
            "Which state had the most pilots per capita than any other US state?\n",
            "What were slaves not allowed to do?\n",
            "Slaves were not allowed to do what?\n",
            "Where does the ferry end to France?\n",
            "Where does the ferry to France terminate?\n"
          ]
        }
      ],
      "source": [
        "for i in range(61000,61010):\n",
        "    print(augmented_squad['train']['question'][i])\n",
        "    print(squad['train']['question'][i])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "6045c440",
      "metadata": {
        "id": "6045c440"
      },
      "outputs": [],
      "source": [
        "def preprocess_function(examples):\n",
        "    questions = [q.strip() for q in examples[\"question\"]]\n",
        "    inputs = tokenizer(\n",
        "        questions,\n",
        "        examples[\"context\"],\n",
        "        max_length=384,\n",
        "        truncation=\"only_second\",\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
        "    answers = examples[\"answers\"]\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "\n",
        "    for i, offset in enumerate(offset_mapping):\n",
        "        answer = answers[i]\n",
        "        start_char = answer[\"answer_start\"][0]\n",
        "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
        "        sequence_ids = inputs.sequence_ids(i)\n",
        "\n",
        "        # Find the start and end of the context\n",
        "        idx = 0\n",
        "        while sequence_ids[idx] != 1:\n",
        "            idx += 1\n",
        "        context_start = idx\n",
        "        while sequence_ids[idx] == 1:\n",
        "            idx += 1\n",
        "        context_end = idx - 1\n",
        "\n",
        "        # If the answer is not fully inside the context, label it (0, 0)\n",
        "        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
        "            start_positions.append(0)\n",
        "            end_positions.append(0)\n",
        "        else:\n",
        "            # Otherwise it's the start and end token positions\n",
        "            idx = context_start\n",
        "            while idx <= context_end and offset[idx][0] <= start_char:\n",
        "                idx += 1\n",
        "            start_positions.append(idx - 1)\n",
        "\n",
        "            idx = context_end\n",
        "            while idx >= context_start and offset[idx][1] >= end_char:\n",
        "                idx -= 1\n",
        "            end_positions.append(idx + 1)\n",
        "\n",
        "    inputs[\"start_positions\"] = start_positions\n",
        "    inputs[\"end_positions\"] = end_positions\n",
        "    return inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "fa710405",
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
        "\n",
        "from transformers import DefaultDataCollator\n",
        "\n",
        "data_collator = DefaultDataCollator()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "7b4d33a6",
      "metadata": {
        "id": "7b4d33a6"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8e56bdc0ab794825ba9051a3ad9e2c5a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/70079 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tokenized_squad = augmented_squad.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=augmented_squad[\"train\"].column_names,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "fa2e6138",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\Jettanat\\anaconda3\\envs\\fuckingneuralnetwork\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n",
            "Trainable parameters: 70\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
        "\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\"./drill04_qa_model\")\n",
        "\n",
        "# Freeze first 2 transformer layers\n",
        "for layer in model.distilbert.transformer.layer[:2]:\n",
        "    for param in layer.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "# Check which parameters are trainable\n",
        "trainable_params = [n for n, p in model.named_parameters() if p.requires_grad]\n",
        "print(f\"Trainable parameters: {len(trainable_params)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "e5d5f920",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Column(['What was the cost of primary and secondary education under Gaddafi?', 'What was the price of early HDTVs?', 'At its peak, the imperialist forces controlled all but how many states in Mexico?', 'Serbo-Croatian is the only Slavic language that uses which two scripts together?', 'What is the third - hot desert in the world?'])\n",
            "Column(['What was the cost of primary and secondary education under Gaddafi?', 'What was the price for early HDTVs?', 'At its zenith, the imperialist forces controlled all but how many states in Mexico?', 'Serbo-Croatian is the only Slavic language to use what two scripts together?', 'What is the third hottest desert in the world?'])\n"
          ]
        }
      ],
      "source": [
        "print(augmented_squad[\"train\"]['question'])\n",
        "print(squad[\"train\"]['question'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec62030e",
      "metadata": {
        "id": "ec62030e",
        "outputId": "9bbffb4c-61f3-4846-e8ea-ac6627e09030"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "Device name: NVIDIA GeForce RTX 3050 Laptop GPU\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Jettanat\\AppData\\Local\\Temp\\ipykernel_34364\\1263073497.py:33: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training from scratch...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='17520' max='17520' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [17520/17520 3:19:35, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.048500</td>\n",
              "      <td>1.098610</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.920500</td>\n",
              "      <td>1.098198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.865000</td>\n",
              "      <td>1.124761</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.773800</td>\n",
              "      <td>1.141018</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# ✅ Use GPU if available\n",
        "import torch\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"There are {torch.cuda.device_count()} GPU(s) available.\")\n",
        "    print(\"Device name:\", torch.cuda.get_device_name(0))\n",
        "    model.to(device)\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"No GPU found, using CPU.\")\n",
        "\n",
        "# ✅ Training configuration (optimized for disk usage)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"drill04+DE_Backtranslated\",       # folder to save model\n",
        "    eval_strategy=\"epoch\",         # correct parameter name\n",
        "    learning_rate=1e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=4,\n",
        "    weight_decay=0.01,\n",
        "    save_strategy=\"epoch\",               # save checkpoint only once per epoch\n",
        "    save_total_limit=1,                  # keep only the last checkpoint\n",
        "    load_best_model_at_end=True,         # optional, keeps best checkpoint\n",
        "    push_to_hub=False,\n",
        "    report_to=\"none\",                    # disable wandb or tensorboard logs\n",
        "    logging_dir=None,                    # avoid creating logging folders\n",
        "    #fp16=True\n",
        ")\n",
        "\n",
        "# ✅ Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_squad[\"train\"],\n",
        "    eval_dataset=tokenized_squad[\"test\"],\n",
        "    tokenizer=tokenizer,                 # fixed from \"processing_class\"\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# ✅ Try to resume training if a checkpoint exists\n",
        "import os\n",
        "last_checkpoint = None\n",
        "if os.path.isdir(training_args.output_dir):\n",
        "    from transformers.trainer_utils import get_last_checkpoint\n",
        "    last_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
        "\n",
        "if last_checkpoint:\n",
        "    print(f\"Resuming training from checkpoint: {last_checkpoint}\")\n",
        "    trainer.train(resume_from_checkpoint=last_checkpoint)\n",
        "else:\n",
        "    print(\"Starting training from scratch...\")\n",
        "    trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "e6f9d3f0",
      "metadata": {
        "id": "e6f9d3f0",
        "outputId": "6d5e86e6-5e1d-4d94-eaf1-e9c919a6184e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('./drill04+DE_Backtranslated\\\\tokenizer_config.json',\n",
              " './drill04+DE_Backtranslated\\\\special_tokens_map.json',\n",
              " './drill04+DE_Backtranslated\\\\vocab.txt',\n",
              " './drill04+DE_Backtranslated\\\\added_tokens.json',\n",
              " './drill04+DE_Backtranslated\\\\tokenizer.json')"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Save the model and tokenizer\n",
        "trainer.save_model(\"./drill04+DE_Backtranslated\")\n",
        "tokenizer.save_pretrained(\"./drill04+DE_Backtranslated\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "551c8bf0",
      "metadata": {
        "id": "551c8bf0",
        "outputId": "b40ff1c3-c7b5-4b97-de3d-a4a5b91288fa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(17520, 17688)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "max_length = 384\n",
        "stride = 128\n",
        "\n",
        "\n",
        "def preprocess_validation_examples(examples):\n",
        "    questions = [q.strip() for q in examples[\"question\"]]\n",
        "    inputs = tokenizer(\n",
        "        questions,\n",
        "        examples[\"context\"],\n",
        "        max_length=max_length,\n",
        "        truncation=\"only_second\",\n",
        "        stride=stride,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
        "    example_ids = []\n",
        "\n",
        "    for i in range(len(inputs[\"input_ids\"])):\n",
        "        sample_idx = sample_map[i]\n",
        "        example_ids.append(examples[\"id\"][sample_idx])\n",
        "\n",
        "        sequence_ids = inputs.sequence_ids(i)\n",
        "        offset = inputs[\"offset_mapping\"][i]\n",
        "        inputs[\"offset_mapping\"][i] = [\n",
        "            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
        "        ]\n",
        "\n",
        "    inputs[\"example_id\"] = example_ids\n",
        "    return inputs\n",
        "\n",
        "validation_dataset = squad[\"test\"].map(\n",
        "    preprocess_validation_examples,\n",
        "    batched=True,\n",
        "    remove_columns=squad[\"test\"].column_names,\n",
        ")\n",
        "len(squad[\"test\"]), len(validation_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "9b91bec1",
      "metadata": {
        "id": "9b91bec1"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "import collections\n",
        "import numpy as np\n",
        "import evaluate\n",
        "metric = evaluate.load(\"squad\")\n",
        "\n",
        "n_best = 20\n",
        "max_answer_length = 30\n",
        "\n",
        "def compute_metrics(start_logits, end_logits, features, examples):\n",
        "    example_to_features = collections.defaultdict(list)\n",
        "    for idx, feature in enumerate(features):\n",
        "        example_to_features[feature[\"example_id\"]].append(idx)\n",
        "\n",
        "    predicted_answers = []\n",
        "    for example in tqdm(examples):\n",
        "        example_id = example[\"id\"]\n",
        "        context = example[\"context\"]\n",
        "        answers = []\n",
        "\n",
        "        # Loop through all features associated with that example\n",
        "        for feature_index in example_to_features[example_id]:\n",
        "            start_logit = start_logits[feature_index]\n",
        "            end_logit = end_logits[feature_index]\n",
        "            offsets = features[feature_index][\"offset_mapping\"]\n",
        "\n",
        "            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
        "            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
        "            for start_index in start_indexes:\n",
        "                for end_index in end_indexes:\n",
        "                    # Skip answers that are not fully in the context\n",
        "                    if offsets[start_index] is None or offsets[end_index] is None:\n",
        "                        continue\n",
        "                    # Skip answers with a length that is either < 0 or > max_answer_length\n",
        "                    if (\n",
        "                        end_index < start_index\n",
        "                        or end_index - start_index + 1 > max_answer_length\n",
        "                    ):\n",
        "                        continue\n",
        "\n",
        "                    answer = {\n",
        "                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
        "                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
        "                    }\n",
        "                    answers.append(answer)\n",
        "\n",
        "        # Select the answer with the best score\n",
        "        if len(answers) > 0:\n",
        "            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
        "            predicted_answers.append(\n",
        "                {\"id\": example_id, \"prediction_text\": best_answer[\"text\"]}\n",
        "            )\n",
        "        else:\n",
        "            predicted_answers.append({\"id\": example_id, \"prediction_text\": \"\"})\n",
        "\n",
        "    theoretical_answers = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in examples]\n",
        "    return metric.compute(predictions=predicted_answers, references=theoretical_answers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "748358f6",
      "metadata": {
        "id": "748358f6",
        "outputId": "c717aa47-93f3-4706-a1f6-798ceff48909"
      },
      "outputs": [
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2a0914c96c0e4043b1ab71dac3a74633",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/17520 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'exact_match': 64.01826484018265, 'f1': 78.06739535271652}"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions, _, _ = trainer.predict(validation_dataset)\n",
        "start_logits, end_logits = predictions\n",
        "compute_metrics(start_logits, end_logits, validation_dataset, squad[\"test\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "ac1dddd9",
      "metadata": {
        "id": "ac1dddd9",
        "outputId": "6ead3be7-cac1-494d-a6b8-1078a3841b0e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Which is blue's car price\n",
            "$20,000 and the red car costs $25,000\n"
          ]
        }
      ],
      "source": [
        "question = \"Which is blue's car price\"\n",
        "context=\"The blue car costs $20,000 and the red car costs $25,000.\"\n",
        "print(question)\n",
        "from transformers import pipeline\n",
        "question_answerer = pipeline(\"question-answering\",\n",
        "                             model=\"./drill04_qa_model\",\n",
        "                            tokenizer=\"./drill04_qa_model\",\n",
        "                            fp16=True)\n",
        "\n",
        "result = question_answerer(question=question, context=context)\n",
        "print(result['answer'])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "fuckingneuralnetwork",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
