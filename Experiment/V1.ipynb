{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a2655ef1",
      "metadata": {
        "id": "a2655ef1",
        "outputId": "2ed74b49-5671-49a9-f7d1-a39878c6f08c"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "squad = load_dataset(\"squad\", split=\"train\")\n",
        "squad = squad.train_test_split(test_size=0.2, seed=42)\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
        "\n",
        "from transformers import DefaultDataCollator\n",
        "\n",
        "data_collator = DefaultDataCollator()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "7c3fec36",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to c:\\Users\\Jettanat\\anaconda3\n",
            "[nltk_data]     \\envs\\fuckingneuralnetwork\\lib\\nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to c:\\Users\\Jettanat\\anaconda3\n",
            "[nltk_data]     \\envs\\fuckingneuralnetwork\\lib\\nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to c:\\Users\n",
            "[nltk_data]     \\Jettanat\\anaconda3\\envs\\fuckingneuralnetwork\\lib\\nltk\n",
            "[nltk_data]     _data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to c:\\Users\n",
            "[nltk_data]     \\Jettanat\\anaconda3\\envs\\fuckingneuralnetwork\\lib\\nltk\n",
            "[nltk_data]     _data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to c:\\Users\\Jettanat\\anaconda3\\e\n",
            "[nltk_data]     nvs\\fuckingneuralnetwork\\lib\\nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')  # optional, sometimes needed for tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "270b544b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('Slavic ', 'NN')]\n"
          ]
        }
      ],
      "source": [
        "from nltk import pos_tag\n",
        "\n",
        "words = ['Slavic ']\n",
        "tags = pos_tag(words)\n",
        "print(tags)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c488136",
      "metadata": {},
      "source": [
        "SYNONYM REPLACEMENT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "c663f78b",
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "from random import shuffle\n",
        "from nltk.corpus import wordnet \n",
        "def get_only_chars(line):\n",
        "\n",
        "    clean_line = \"\"\n",
        "\n",
        "    line = line.replace(\"â€™\", \"\")\n",
        "    line = line.replace(\"'\", \"\")\n",
        "    line = line.replace(\"-\", \" \") #replace hyphens with spaces\n",
        "    line = line.replace(\"\\t\", \" \")\n",
        "    line = line.replace(\"\\n\", \" \")\n",
        "    line = line.lower()\n",
        "\n",
        "    for char in line:\n",
        "        if char in 'qwertyuiopasdfghjklzxcvbnm ':\n",
        "            clean_line += char\n",
        "        else:\n",
        "            clean_line += ' '\n",
        "\n",
        "    clean_line = re.sub(' +',' ',clean_line) #delete extra spaces\n",
        "    if clean_line[0] == ' ':\n",
        "        clean_line = clean_line[1:]\n",
        "    return clean_line\n",
        "\n",
        "stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', \n",
        "\t\t\t'ours', 'ourselves', 'you', 'your', 'yours', \n",
        "\t\t\t'yourself', 'yourselves', 'he', 'him', 'his', \n",
        "\t\t\t'himself', 'she', 'her', 'hers', 'herself', \n",
        "\t\t\t'it', 'its', 'itself', 'they', 'them', 'their', \n",
        "\t\t\t'theirs', 'themselves', 'what', 'which', 'who', \n",
        "\t\t\t'whom', 'this', 'that', 'these', 'those', 'am', \n",
        "\t\t\t'is', 'are', 'was', 'were', 'be', 'been', 'being', \n",
        "\t\t\t'have', 'has', 'had', 'having', 'do', 'does', 'did',\n",
        "\t\t\t'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or',\n",
        "\t\t\t'because', 'as', 'until', 'while', 'of', 'at', \n",
        "\t\t\t'by', 'for', 'with', 'about', 'against', 'between',\n",
        "\t\t\t'into', 'through', 'during', 'before', 'after', \n",
        "\t\t\t'above', 'below', 'to', 'from', 'up', 'down', 'in',\n",
        "\t\t\t'out', 'on', 'off', 'over', 'under', 'again', \n",
        "\t\t\t'further', 'then', 'once', 'here', 'there', 'when', \n",
        "\t\t\t'where', 'why', 'how', 'all', 'any', 'both', 'each', \n",
        "\t\t\t'few', 'more', 'most', 'other', 'some', 'such', 'no', \n",
        "\t\t\t'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', \n",
        "\t\t\t'very', 's', 't', 'can', 'will', 'just', 'don', \n",
        "\t\t\t'should', 'now', '']\n",
        "def synonym_replacement(words, n):\n",
        "    new_words = words.copy()\n",
        "    \n",
        "    # POS tagging\n",
        "    tags = pos_tag(words)\n",
        "    \n",
        "    # Candidate words: skip stopwords, proper nouns (NNP/NNPS), numbers\n",
        "    candidates = [\n",
        "        word for word, pos in tags\n",
        "        if word.lower() not in stop_words\n",
        "        and pos not in [\"NNP\", \"NNPS\"]   # skip proper nouns\n",
        "        and not word.isdigit()           # skip numbers\n",
        "    ]\n",
        "    \n",
        "    random.shuffle(candidates)\n",
        "    num_replaced = 0\n",
        "    \n",
        "    for word in candidates:\n",
        "        synonyms = get_synonyms(word)\n",
        "        if synonyms:\n",
        "            synonym = random.choice(synonyms)\n",
        "            new_words = [synonym if w == word else w for w in new_words]\n",
        "            num_replaced += 1\n",
        "        if num_replaced >= n:\n",
        "            break\n",
        "\n",
        "    # Reconstruct sentence\n",
        "    sentence = ' '.join(new_words)\n",
        "    new_words = sentence.split(' ')\n",
        "\n",
        "    return new_words\n",
        "\n",
        "def get_synonyms(word):\n",
        "\tsynonyms = set()\n",
        "\tfor syn in wordnet.synsets(word): \n",
        "\t\tfor l in syn.lemmas(): \n",
        "\t\t\tsynonym = l.name().replace(\"_\", \" \").replace(\"-\", \" \").lower()\n",
        "\t\t\tsynonym = \"\".join([char for char in synonym if char in ' qwertyuiopasdfghjklzxcvbnm'])\n",
        "\t\t\tsynonyms.add(synonym) \n",
        "\tif word in synonyms:\n",
        "\t\tsynonyms.remove(word)\n",
        "\treturn list(synonyms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "b5775ba3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original examples: 70079\n",
            "Augmented examples: 70079\n",
            "\n",
            "ðŸŸ© Original:  What was the price for early HDTVs?\n",
            "ðŸŸ¦ Augmented: what was the toll for early hdtvs \n",
            "--------------------------------------------------------------------------------\n",
            "ðŸŸ© Original:  At its zenith, the imperialist forces controlled all but how many states in Mexico?\n",
            "ðŸŸ¦ Augmented: at its zenith the imperialist strength controlled all but how many states in mexico \n",
            "--------------------------------------------------------------------------------\n",
            "ðŸŸ© Original:  Serbo-Croatian is the only Slavic language to use what two scripts together?\n",
            "ðŸŸ¦ Augmented: serbo croatian is the only slavic language to use what two script together \n",
            "--------------------------------------------------------------------------------\n",
            "ðŸŸ© Original:  What is the third hottest desert in the world?\n",
            "ðŸŸ¦ Augmented: what is the third blistering desert in the world \n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import re\n",
        "from nltk.corpus import wordnet\n",
        "from copy import deepcopy\n",
        "\n",
        "# squad = load_dataset(\"squad\")\n",
        "examples_dict = squad['train'][:]\n",
        "\n",
        "# Convert column-oriented dict to list of row dicts\n",
        "examples = [dict(zip(examples_dict.keys(), values)) for values in zip(*examples_dict.values())]\n",
        "\n",
        "# Augmentation parameters\n",
        "N_AUGMENTATIONS_PER_SAMPLE = 1\n",
        "MAX_REPLACEMENTS = 1\n",
        "random.seed(1)\n",
        "\n",
        "augmented_samples = []\n",
        "\n",
        "for ex in examples:\n",
        "    original_q = ex['question']\n",
        "    cleaned = get_only_chars(original_q)\n",
        "    words = cleaned.split(' ')\n",
        "\n",
        "    # Create augmented versions\n",
        "    for i in range(N_AUGMENTATIONS_PER_SAMPLE):\n",
        "        augmented_words = synonym_replacement(words, MAX_REPLACEMENTS)\n",
        "        augmented_q = ' '.join(augmented_words)\n",
        "\n",
        "        new_sample = deepcopy(ex)\n",
        "        new_sample['id'] = ex['id'] + f\"_aug_{i}\"\n",
        "        new_sample['question'] = augmented_q\n",
        "        augmented_samples.append(new_sample)\n",
        "\n",
        "# Print results\n",
        "print(f\"Original examples: {len(examples)}\")\n",
        "print(f\"Augmented examples: {len(augmented_samples)}\\n\")\n",
        "\n",
        "for i, ex in enumerate(examples[1:5]):\n",
        "    print(f\"ðŸŸ© Original:  {ex['question']}\")\n",
        "    for aug in augmented_samples:\n",
        "        if aug['id'].startswith(ex['id']):\n",
        "            print(f\"ðŸŸ¦ Augmented: {aug['question']}\")\n",
        "    print('-' * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "b49c4a8f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What is the third hottest desert in the world?\n",
            "what is the third blistering desert in the world \n"
          ]
        }
      ],
      "source": [
        "from datasets import Dataset, DatasetDict\n",
        "import pandas as pd\n",
        "\n",
        "# Convert augmented samples (list of dicts) to a Dataset\n",
        "augmented_train_dataset = Dataset.from_pandas(pd.DataFrame(augmented_samples))\n",
        "\n",
        "# Keep original test dataset\n",
        "test_dataset = squad['test']\n",
        "\n",
        "# Create new DatasetDict\n",
        "augmented_squad = DatasetDict({\n",
        "    'train': augmented_train_dataset,\n",
        "    'test': test_dataset\n",
        "})\n",
        "\n",
        "# Check\n",
        "print(squad['train']['question'][4])\n",
        "print(augmented_squad['train']['question'][4])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "6045c440",
      "metadata": {
        "id": "6045c440"
      },
      "outputs": [],
      "source": [
        "def preprocess_function(examples):\n",
        "    questions = [q.strip() for q in examples[\"question\"]]\n",
        "    inputs = tokenizer(\n",
        "        questions,\n",
        "        examples[\"context\"],\n",
        "        max_length=384,\n",
        "        truncation=\"only_second\",\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
        "    answers = examples[\"answers\"]\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "\n",
        "    for i, offset in enumerate(offset_mapping):\n",
        "        answer = answers[i]\n",
        "        start_char = answer[\"answer_start\"][0]\n",
        "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
        "        sequence_ids = inputs.sequence_ids(i)\n",
        "\n",
        "        # Find the start and end of the context\n",
        "        idx = 0\n",
        "        while sequence_ids[idx] != 1:\n",
        "            idx += 1\n",
        "        context_start = idx\n",
        "        while sequence_ids[idx] == 1:\n",
        "            idx += 1\n",
        "        context_end = idx - 1\n",
        "\n",
        "        # If the answer is not fully inside the context, label it (0, 0)\n",
        "        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
        "            start_positions.append(0)\n",
        "            end_positions.append(0)\n",
        "        else:\n",
        "            # Otherwise it's the start and end token positions\n",
        "            idx = context_start\n",
        "            while idx <= context_end and offset[idx][0] <= start_char:\n",
        "                idx += 1\n",
        "            start_positions.append(idx - 1)\n",
        "\n",
        "            idx = context_end\n",
        "            while idx >= context_start and offset[idx][1] >= end_char:\n",
        "                idx -= 1\n",
        "            end_positions.append(idx + 1)\n",
        "\n",
        "    inputs[\"start_positions\"] = start_positions\n",
        "    inputs[\"end_positions\"] = end_positions\n",
        "    return inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "7b4d33a6",
      "metadata": {
        "id": "7b4d33a6"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "56aa4c3b1d2c497f98188cf6ec75cb41",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/70079 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tokenized_squad = augmented_squad.map(preprocess_function, batched=True, remove_columns=augmented_squad[\"train\"].column_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "fa2e6138",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\Jettanat\\anaconda3\\envs\\fuckingneuralnetwork\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n",
            "Trainable parameters: 70\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
        "\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\"./drill04_qa_model\")\n",
        "\n",
        "# Freeze first 2 transformer layers\n",
        "for layer in model.distilbert.transformer.layer[:2]:\n",
        "    for param in layer.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "# Check which parameters are trainable\n",
        "trainable_params = [n for n, p in model.named_parameters() if p.requires_grad]\n",
        "print(f\"Trainable parameters: {len(trainable_params)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "ec62030e",
      "metadata": {
        "id": "ec62030e",
        "outputId": "9bbffb4c-61f3-4846-e8ea-ac6627e09030"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "Device name: NVIDIA GeForce RTX 3050 Laptop GPU\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Jettanat\\AppData\\Local\\Temp\\ipykernel_33024\\1267413822.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training from scratch...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='17520' max='17520' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [17520/17520 3:10:40, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.139700</td>\n",
              "      <td>1.125495</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.990200</td>\n",
              "      <td>1.126850</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.912400</td>\n",
              "      <td>1.152648</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.820300</td>\n",
              "      <td>1.169969</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# âœ… Use GPU if available\n",
        "import torch\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"There are {torch.cuda.device_count()} GPU(s) available.\")\n",
        "    print(\"Device name:\", torch.cuda.get_device_name(0))\n",
        "    model.to(device)\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"No GPU found, using CPU.\")\n",
        "\n",
        "# âœ… Training configuration (optimized for disk usage)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"drill04+QuestionSynonym\",       # folder to save model\n",
        "    eval_strategy=\"epoch\",         # correct parameter name\n",
        "    learning_rate=1e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=4,\n",
        "    weight_decay=0.01,\n",
        "    save_strategy=\"epoch\",               # save checkpoint only once per epoch\n",
        "    save_total_limit=1,                  # keep only the last checkpoint\n",
        "    load_best_model_at_end=True,         # optional, keeps best checkpoint\n",
        "    push_to_hub=False,\n",
        "    report_to=\"none\",                    # disable wandb or tensorboard logs\n",
        "    logging_dir=None,                    # avoid creating logging folders\n",
        ")\n",
        "\n",
        "# âœ… Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_squad[\"train\"],\n",
        "    eval_dataset=tokenized_squad[\"test\"],\n",
        "    tokenizer=tokenizer,                 # fixed from \"processing_class\"\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# âœ… Try to resume training if a checkpoint exists\n",
        "import os\n",
        "last_checkpoint = None\n",
        "if os.path.isdir(training_args.output_dir):\n",
        "    from transformers.trainer_utils import get_last_checkpoint\n",
        "    last_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
        "\n",
        "if last_checkpoint:\n",
        "    print(f\"Resuming training from checkpoint: {last_checkpoint}\")\n",
        "    trainer.train(resume_from_checkpoint=last_checkpoint)\n",
        "else:\n",
        "    print(\"Starting training from scratch...\")\n",
        "    trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "e6f9d3f0",
      "metadata": {
        "id": "e6f9d3f0",
        "outputId": "6d5e86e6-5e1d-4d94-eaf1-e9c919a6184e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('./drill04+QuestionSynonym\\\\tokenizer_config.json',\n",
              " './drill04+QuestionSynonym\\\\special_tokens_map.json',\n",
              " './drill04+QuestionSynonym\\\\vocab.txt',\n",
              " './drill04+QuestionSynonym\\\\added_tokens.json',\n",
              " './drill04+QuestionSynonym\\\\tokenizer.json')"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Save the model and tokenizer\n",
        "trainer.save_model(\"./drill04+QuestionSynonym\")\n",
        "tokenizer.save_pretrained(\"./drill04+QuestionSynonym\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "551c8bf0",
      "metadata": {
        "id": "551c8bf0",
        "outputId": "b40ff1c3-c7b5-4b97-de3d-a4a5b91288fa"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "75d0c385098b4972b5b64c783ded5d1c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/17520 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "(17520, 17688)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "max_length = 384\n",
        "stride = 128\n",
        "\n",
        "\n",
        "def preprocess_validation_examples(examples):\n",
        "    questions = [q.strip() for q in examples[\"question\"]]\n",
        "    inputs = tokenizer(\n",
        "        questions,\n",
        "        examples[\"context\"],\n",
        "        max_length=max_length,\n",
        "        truncation=\"only_second\",\n",
        "        stride=stride,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
        "    example_ids = []\n",
        "\n",
        "    for i in range(len(inputs[\"input_ids\"])):\n",
        "        sample_idx = sample_map[i]\n",
        "        example_ids.append(examples[\"id\"][sample_idx])\n",
        "\n",
        "        sequence_ids = inputs.sequence_ids(i)\n",
        "        offset = inputs[\"offset_mapping\"][i]\n",
        "        inputs[\"offset_mapping\"][i] = [\n",
        "            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
        "        ]\n",
        "\n",
        "    inputs[\"example_id\"] = example_ids\n",
        "    return inputs\n",
        "\n",
        "validation_dataset = squad[\"test\"].map(\n",
        "    preprocess_validation_examples,\n",
        "    batched=True,\n",
        "    remove_columns=squad[\"test\"].column_names,\n",
        ")\n",
        "len(squad[\"test\"]), len(validation_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "9b91bec1",
      "metadata": {
        "id": "9b91bec1"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "import collections\n",
        "import numpy as np\n",
        "import evaluate\n",
        "metric = evaluate.load(\"squad\")\n",
        "\n",
        "n_best = 20\n",
        "max_answer_length = 30\n",
        "\n",
        "def compute_metrics(start_logits, end_logits, features, examples):\n",
        "    example_to_features = collections.defaultdict(list)\n",
        "    for idx, feature in enumerate(features):\n",
        "        example_to_features[feature[\"example_id\"]].append(idx)\n",
        "\n",
        "    predicted_answers = []\n",
        "    for example in tqdm(examples):\n",
        "        example_id = example[\"id\"]\n",
        "        context = example[\"context\"]\n",
        "        answers = []\n",
        "\n",
        "        # Loop through all features associated with that example\n",
        "        for feature_index in example_to_features[example_id]:\n",
        "            start_logit = start_logits[feature_index]\n",
        "            end_logit = end_logits[feature_index]\n",
        "            offsets = features[feature_index][\"offset_mapping\"]\n",
        "\n",
        "            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
        "            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
        "            for start_index in start_indexes:\n",
        "                for end_index in end_indexes:\n",
        "                    # Skip answers that are not fully in the context\n",
        "                    if offsets[start_index] is None or offsets[end_index] is None:\n",
        "                        continue\n",
        "                    # Skip answers with a length that is either < 0 or > max_answer_length\n",
        "                    if (\n",
        "                        end_index < start_index\n",
        "                        or end_index - start_index + 1 > max_answer_length\n",
        "                    ):\n",
        "                        continue\n",
        "\n",
        "                    answer = {\n",
        "                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
        "                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
        "                    }\n",
        "                    answers.append(answer)\n",
        "\n",
        "        # Select the answer with the best score\n",
        "        if len(answers) > 0:\n",
        "            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
        "            predicted_answers.append(\n",
        "                {\"id\": example_id, \"prediction_text\": best_answer[\"text\"]}\n",
        "            )\n",
        "        else:\n",
        "            predicted_answers.append({\"id\": example_id, \"prediction_text\": \"\"})\n",
        "\n",
        "    theoretical_answers = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in examples]\n",
        "    return metric.compute(predictions=predicted_answers, references=theoretical_answers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "748358f6",
      "metadata": {
        "id": "748358f6",
        "outputId": "c717aa47-93f3-4706-a1f6-798ceff48909"
      },
      "outputs": [
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9a22dcf222fc4e999239b9e9f865397e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/17520 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'exact_match': 63.06506849315068, 'f1': 76.98337549190808}"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions, _, _ = trainer.predict(validation_dataset)\n",
        "start_logits, end_logits = predictions\n",
        "compute_metrics(start_logits, end_logits, validation_dataset, squad[\"test\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "ac1dddd9",
      "metadata": {
        "id": "ac1dddd9",
        "outputId": "6ead3be7-cac1-494d-a6b8-1078a3841b0e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "which car costs $20000\n",
            "red car\n"
          ]
        }
      ],
      "source": [
        "question = 'which car costs $20000'\n",
        "context=\"The blue car costs $20,000 and the red car costs $25,000.\"\n",
        "print(question)\n",
        "from transformers import pipeline\n",
        "question_answerer = pipeline(\"question-answering\",\n",
        "                             model=\"./drill04+QuestionSynonym\",\n",
        "                            tokenizer=\"./drill04+QuestionSynonym\",\n",
        "                            fp16=True)\n",
        "\n",
        "result = question_answerer(question=question, context=context)\n",
        "print(result['answer'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "3ce570f4",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['what was the toll for early hdtvs ',\n",
              " 'at its zenith the imperialist strength controlled all but how many states in mexico ',\n",
              " 'serbo croatian is the only slavic language to use what two script together ',\n",
              " 'what is the third blistering desert in the world ',\n",
              " 'what is the nearest port to angel helena ',\n",
              " 'what did darwin introduce that changed classification system ',\n",
              " 'how many legislative assemblies are held each yr ',\n",
              " 'when was thatcher appointed prime parson ',\n",
              " 'what type of hold do communitarians tend to support ',\n",
              " 'in what year did sir john herschel publish his work on infrared radiation ',\n",
              " 'who sack the petition of the jewish community to stop the abuse of them ',\n",
              " 'what nasser related film set a box agency record ',\n",
              " 'in foot how much of the dawlish sea wall was destroyed ',\n",
              " 'how very much was the average cost of hospital stays for asthma related issues for children ',\n",
              " 'for what state artifacts did the vestal care ',\n",
              " 'with what belorussian metropolis does kathmandu have a relationship ',\n",
              " 'who was the author of conversations on the plurality of humans ',\n",
              " 'when did maker north take office ',\n",
              " 'in addition to the evening amusement industry people from what industry often oppose daylight saving ',\n",
              " 'what effect was ascend salinity having on the agricultural productivity of the sumerian lands ',\n",
              " 'what was the go out that the armistice of mudros came to be signed ',\n",
              " 'who collaborated with freddie mercury on the sung under pressure ',\n",
              " 'what was the cite of shen fus memoir ',\n",
              " 'in what field did montini obtain a doctors degree in ',\n",
              " 'the term pre columbian technically refer to which era ',\n",
              " 'which north american version of football calls for actor per side on the field ',\n",
              " 'what is the public figure of the highest selling tabloid daily newspaper in the united kingdom ',\n",
              " 'in what jurisdiction were internet archive counterclaims filed ',\n",
              " 'israel is a leading country of what exploitation ',\n",
              " 'in what class was pre and extra marital sex outlawed by the catholic church ',\n",
              " 'what time magazine collapse attended yale ',\n",
              " 'who put back lord rosebery the year following his appointment ',\n",
              " 'there are more fires in the fall and winter because people burn more standard candle and turn what on to keep warm ',\n",
              " 'what would be required for the human body to potentially have a opportunity to create a complete protein source ',\n",
              " 'who denote to the classical attic dialect stops by the three distinctions ',\n",
              " 'after alexanders empire was split what part did seleucus i nicator receive ',\n",
              " 'in what italian small town was giovanni battista montini born ',\n",
              " 'how much of the spanish population are barcelona booster ',\n",
              " 'how many years is the deal with legendary pictures signify to run ',\n",
              " 'how many people were killed at the charlie hebdo lash out ',\n",
              " 'the special clerical court is accountable to only which eubstance ',\n",
              " 'who was taken to the hospital after the clear nine show ',\n",
              " 'what do enraptured mystics hope to achieve ',\n",
              " 'what american university has a gravid campus in mexico city ',\n",
              " 'what variety of stamp is required to hunt migratory waterfowl ',\n",
              " 'if a patient has some symptoms associated with tb after what clock period should tuberculosis be diagnostically considered ',\n",
              " 'how long did gamesradar read it took sony to turn the internet against the playstation ',\n",
              " 'what metaphysical concept were included in the vedanta school ',\n",
              " 'where did the metro department store originally set out ',\n",
              " 'what time did jesus drop dead per mark ',\n",
              " 'how much has new types of internet journalism supercede what has been lost in the press ',\n",
              " 'where did the nazi and russian leaders meet to discuss what to do with republic of poland ',\n",
              " 'what is the sort of circular imperfection in a piece of woods called ',\n",
              " 'how many times has the university of kansas won a national championship in mens hoops ',\n",
              " 'what radio station format plays mild ac hot ac disco and dance ',\n",
              " 'when did building begin on the philadelphia city foyer ',\n",
              " 'what is the name of the standard related to the materials used in solar water heater ',\n",
              " 'what is the driving force for maintaining an vestibular sense within an ecosystem ',\n",
              " 'what is required in order for ironware to be compromised ',\n",
              " 'what buddhist belief says that buddhas come unmatchable at a time and not within other eras ',\n",
              " 'during what century did scandinavians begin to descend in england ',\n",
              " 'what are a couple examples of non summer music fete in portugal ',\n",
              " 'victoria blamed gladstone for the decease of who ',\n",
              " 'as of may what was robert pattons job deed ',\n",
              " 'this produces many modes of quiver that occur simultaneously ',\n",
              " 'growth of solar water heating system development has averaged how much per year since ',\n",
              " 'when was the sedition move passed ',\n",
              " 'what political political party does aung san suu kyi belong to ',\n",
              " 'which two asiatic countries have started to adopt the rule of law ',\n",
              " 'what is the germ of the population data ',\n",
              " 'how long was benjamin disraeli in office ',\n",
              " 'when did tito first visit bharat ',\n",
              " 'when was the animation learning center accredited as a animation building ',\n",
              " 'who afford the imperial institute in ',\n",
              " 'what is the process called of encoding at the source of the information before its processed ',\n",
              " 'londons public transportation is supervise by which agency ',\n",
              " 'when was the war of the worlds radio set broadcast ',\n",
              " 'in the us the fashion during this time of neoclassical empire was known as what ',\n",
              " 'unlike the famicom the nes controllers possessed what reproducible feature ',\n",
              " 'in what yr was wayman v southard tried by the u s supreme court ',\n",
              " 'in what country was the framework constitution of in effect for only one year ',\n",
              " 'what was madonnas debut single call off ',\n",
              " 'what does pra a perform tr s poderes mean ',\n",
              " 'in what ii major conflicts did portugal engage in during the th century ',\n",
              " 'in how many countries does husk have operations ',\n",
              " 'when did the british government take over the royal niger river companys territory ',\n",
              " 'what does itu roentgen stand for ',\n",
              " 'when was government phonemics first seen ',\n",
              " 'besides melanesia which country sent many workers for the cocoa and rubber plantations in samoan islands ',\n",
              " 'from what nation did nasser put up the independence of tunisia algeria and morocco ',\n",
              " 'what is systematically related to the numerate of species ',\n",
              " 'what film festival does the times financial backing that is also supported alongside the british film institute ',\n",
              " 'how much did player jackie chan donate ',\n",
              " 'theres a group of military officer in the congregation what other names are they also known by ',\n",
              " 'what type of catholicism is armenias church service part of ',\n",
              " 'what is the new top enlisted grade in the usaf ',\n",
              " 'who bring together the finalists making it a top ',\n",
              " 'in kilometre what is the distance from plymouth to exeter ',\n",
              " 'what land does contact serve to protect ']"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "augmented_train_dataset['question'][1:100]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "fuckingneuralnetwork",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
